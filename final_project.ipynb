{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchinfo import summary\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "Name: \"IDL Project\"\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "root: \"dataset.csv\"          \n",
    "\n",
    "NUM_WORKERS: 4\n",
    "subset: 1       \n",
    "batch_size: 1\n",
    "\n",
    "embed_dropout: 0.1\n",
    "\n",
    "###### Encoder Specs -------------------------------------------------------------\n",
    "enc_dropout: 0.15\n",
    "enc_num_layers: 1\n",
    "enc_num_heads: 8\n",
    "\n",
    "###### Decoder Specs -------------------------------------------------------------\n",
    "dec_dropout: 0.15\n",
    "dec_num_layers: 1\n",
    "dec_num_heads: 8\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "d_model: 256\n",
    "d_ff: 1024\n",
    "\n",
    "###### Base Parameters -----------------------------------------------------------\n",
    "use_wandb: False\n",
    "use_ctc: False\n",
    "ctc_weight: 0.3\n",
    "optimizer: \"AdamW\" # Adam, AdamW, SGD\n",
    "momentum: 0.0\n",
    "nesterov: True\n",
    "learning_rate: 2E-4\n",
    "scheduler: \"CosineAnnealing\" # ['ReduceLR', 'CosineAnnealing']\n",
    "factor: 0.2\n",
    "patience: 2\n",
    "epochs: 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "  config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  # Change model if needed\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 csv_path: str, \n",
    "                 isTrainPartition: bool, \n",
    "                 tokenizer, \n",
    "                 subset: float = 1.0,\n",
    "                 eos = 1,\n",
    "                 sos = 1,\n",
    "                 pad = 0\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file.\n",
    "            isTrainPartition (bool): Whether the dataset is for training or validation.\n",
    "            tokenizer: Tokenizer instance to tokenize text.\n",
    "            subset (float): Fraction of the dataset to use (0.0 to 1.0).\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_token = eos\n",
    "        self.sos_token = sos\n",
    "        self.pad_token = pad\n",
    "        \n",
    "        # Load data from CSV\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        split_idx = int(len(self.data) * 0.8)\n",
    "        if isTrainPartition:\n",
    "            self.data = self.data.iloc[:split_idx]\n",
    "        else:\n",
    "            self.data = self.data.iloc[split_idx:]\n",
    "\n",
    "        # Apply subset fraction\n",
    "        subset_idx = int(len(self.data) * subset)\n",
    "        self.data = self.data.iloc[:subset_idx]\n",
    "\n",
    "        self.length = len(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        correct_text = row['Correct Text']\n",
    "        noisy_text = row['Noisy Text']\n",
    "\n",
    "        # Tokenize texts\n",
    "        correct_tokens = self.tokenizer.encode(correct_text)\n",
    "        noisy_tokens = self.tokenizer.encode(noisy_text)\n",
    "\n",
    "        # Prepare input, target, and golden sequences\n",
    "        input_seq = torch.LongTensor([self.sos_token] + noisy_tokens + [self.sos_token])\n",
    "        target_seq = torch.LongTensor([self.sos_token] + correct_tokens)\n",
    "        golden_seq = torch.LongTensor(correct_tokens + [self.sos_token])\n",
    "\n",
    "        return input_seq, target_seq, golden_seq\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Collate function for padding and batching.\"\"\"\n",
    "        input_seqs = [item[0] for item in batch]\n",
    "        target_seqs = [item[1] for item in batch]\n",
    "        golden_seqs = [item[2] for item in batch]\n",
    "\n",
    "        # Pad sequences\n",
    "        input_seqs_padded = pad_sequence(input_seqs, batch_first=True, padding_value=self.pad_token)\n",
    "        target_seqs_padded = pad_sequence(target_seqs, batch_first=True, padding_value=self.pad_token)\n",
    "        golden_seqs_padded = pad_sequence(golden_seqs, batch_first=True, padding_value=self.pad_token)\n",
    "\n",
    "        # Get sequence lengths\n",
    "        input_lengths = torch.tensor([len(seq) for seq in input_seqs])\n",
    "        target_lengths = torch.tensor([len(seq) for seq in target_seqs])\n",
    "        golden_lengths = torch.tensor([len(seq) for seq in golden_seqs])\n",
    "\n",
    "        return input_seqs_padded, target_seqs_padded, golden_seqs_padded, input_lengths, target_lengths, golden_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset   = TextDataset(\n",
    "    csv_path        = config['root'],\n",
    "\n",
    "    tokenizer   = tokenizer,\n",
    "    isTrainPartition = True,\n",
    "    subset      = config['subset'],\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset     =  TextDataset(\n",
    "    csv_path        = config['root'],\n",
    "    tokenizer   = tokenizer,\n",
    "    isTrainPartition = False,\n",
    "    subset      = config['subset'],\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config[\"batch_size\"],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['NUM_WORKERS'],\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config[\"batch_size\"],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['NUM_WORKERS'],\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paired Data Stats: \n",
      "No. of Train Feats   : 800\n",
      "Batch Size           : 32\n",
      "Train Batches        : 25\n",
      "Val Batches          : 7\n",
      "\n",
      "Checking the Shapes of the Data --\n",
      "\n",
      "x_pad shape:\t\ttorch.Size([32, 124])\n",
      "x_len shape:\t\ttorch.Size([32])\n",
      "y_shifted_pad shape:\ttorch.Size([32, 121])\n",
      "y_golden_pad shape:\ttorch.Size([32, 121])\n",
      "y_len shape:\t\ttorch.Size([32])\n",
      "\n",
      "Transcript Shifted: [unused1] [CLS] Kwita ku mirire n [UNK] ubuzima bwo mu mutwe ni ingenzi mu kurushaho kugira ubuzima bwiza. Guharanira gutunga intungamubiri zikwiranye n [UNK] umubiri, gukora imyitozo, no kugira umutima utuje bigira ingaruka nziza ku buzima bw [UNK] umubiri. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Transcript Golden: [CLS] Kwita ku mirire n [UNK] ubuzima bwo mu mutwe ni ingenzi mu kurushaho kugira ubuzima bwiza. Guharanira gutunga intungamubiri zikwiranye n [UNK] umubiri, gukora imyitozo, no kugira umutima utuje bigira ingaruka nziza ku buzima bw [UNK] umubiri. [SEP] [unused1] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "\n",
      "Verifying Datasets\n",
      "Loaded Path:  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying train Dataset: 100%|██████████| 25/25 [00:00<00:00, 67.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Feat Length in Dataset       : 124\n",
      "Maximum Transcript Length in Dataset : 121\n",
      "Loaded Path:  val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying val Dataset: 100%|██████████| 7/7 [00:00<00:00, 21.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Feat Length in Dataset       : 39\n",
      "Maximum Transcript Length in Dataset : 37\n",
      "Maximum Feat. Length in Entire Dataset      : 124\n",
      "Maximum Transcript Length in Entire Dataset : 121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1359"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verify_dataset(dataloader, partition):\n",
    "    print(\"Loaded Path: \", partition)\n",
    "    max_len_feat = 0\n",
    "    max_len_t    = 0  \n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Verifying {partition} Dataset\"):\n",
    "      try:\n",
    "        x_pad, y_shifted_pad, y_golden_pad, x_len, y_len, gold_len = batch\n",
    "\n",
    "        len_x = x_pad.shape[1]\n",
    "        if len_x > max_len_feat:\n",
    "            max_len_feat = len_x\n",
    "\n",
    "\n",
    "        len_y = y_shifted_pad.shape[1]\n",
    "        if len_y > max_len_t:\n",
    "            max_len_t = len_y\n",
    "\n",
    "      except:\n",
    "      \n",
    "        y_shifted_pad, y_golden_pad, y_len = batch\n",
    "\n",
    "        len_y = y_shifted_pad.shape[1]\n",
    "        if len_y > max_len_t:\n",
    "            max_len_t = len_y\n",
    "\n",
    "\n",
    "    print(f\"Maximum Feat Length in Dataset       : {max_len_feat}\")\n",
    "    print(f\"Maximum Transcript Length in Dataset : {max_len_t}\")\n",
    "    return max_len_feat, max_len_t\n",
    "\n",
    "print('')\n",
    "print(\"Paired Data Stats: \")\n",
    "print(f\"No. of Train Feats   : {train_dataset.__len__()}\")\n",
    "print(f\"Batch Size           : {config['batch_size']}\")\n",
    "print(f\"Train Batches        : {train_loader.__len__()}\")\n",
    "print(f\"Val Batches          : {val_loader.__len__()}\")\n",
    "print('')\n",
    "print(\"Checking the Shapes of the Data --\\n\")\n",
    "for batch in train_loader:\n",
    "    x_pad, y_shifted_pad, y_golden_pad, x_len, y_len, y_golden_len = batch\n",
    "    print(f\"x_pad shape:\\t\\t{x_pad.shape}\")\n",
    "    print(f\"x_len shape:\\t\\t{x_len.shape}\")\n",
    "    print(f\"y_shifted_pad shape:\\t{y_shifted_pad.shape}\")\n",
    "    print(f\"y_golden_pad shape:\\t{y_golden_pad.shape}\")\n",
    "    print(f\"y_len shape:\\t\\t{y_len.shape}\\n\")\n",
    "\n",
    "    # convert one transcript to text\n",
    "    transcript = train_dataset.tokenizer.decode(y_shifted_pad[0].tolist())\n",
    "    print(f\"Transcript Shifted: {transcript}\")\n",
    "    transcript = train_dataset.tokenizer.decode(y_golden_pad[0].tolist())\n",
    "    print(f\"Transcript Golden: {transcript}\")\n",
    "    break\n",
    "print('')\n",
    "\n",
    "\n",
    "print(\"\\n\\nVerifying Datasets\")\n",
    "max_train_input, max_train_transcript = verify_dataset(train_loader, 'train')\n",
    "max_val_input, max_val_transcript     = verify_dataset(val_loader,   'val')\n",
    "\n",
    "\n",
    "MAX_INP_LEN = max(max_train_input, max_val_input)\n",
    "MAX_TRANS_LEN  = max(max_train_transcript, max_val_transcript)\n",
    "print(f\"Maximum Feat. Length in Entire Dataset      : {MAX_INP_LEN}\")\n",
    "print(f\"Maximum Transcript Length in Entire Dataset : {MAX_TRANS_LEN}\")\n",
    "print('')\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    ''' Position Encoding from Attention Is All You Need Paper '''\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize a tensor to hold the positional encodings\n",
    "        pe          = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Create a tensor representing the positions (0 to max_len-1)\n",
    "        position    = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate the division term for the sine and cosine functions\n",
    "        # This term creates a series of values that decrease geometrically, used to generate varying frequencies for positional encodings\n",
    "        div_term    = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Compute the positional encodings using sine and cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Reshape the positional encodings tensor and make it a buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "      return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PadMask(padded_input, input_lengths=None, pad_idx=None):\n",
    "    \"\"\" Create a mask to identify non-padding positions.\n",
    "\n",
    "    Args:\n",
    "        padded_input: The input tensor with padding, shape (N, T, ...) or (N, T).\n",
    "        input_lengths: Optional, the actual lengths of each sequence before padding, shape (N,).\n",
    "        pad_idx: Optional, the index used for padding tokens.\n",
    "\n",
    "    Returns:\n",
    "        A mask tensor with shape (N, T), where non-padding positions are marked with 1 and padding positions are marked with 0.\n",
    "    \"\"\"\n",
    "\n",
    "   # If input is a 2D tensor (N/, T), add an extra dimension\n",
    "    if padded_input.dim() == 2:\n",
    "        padded_input = padded_input.unsqueeze(-1)\n",
    "\n",
    "    # Create the mask\n",
    "    if input_lengths is not None:\n",
    "        # If lengths are provided, use them to create the mask\n",
    "        N, T, _ = padded_input.shape\n",
    "        mask = torch.ones((N, T), dtype=torch.bool)  # Initialize mask to all True (padding)\n",
    "\n",
    "        for i in range(N):\n",
    "            mask[i, :input_lengths[i]] = False  # Set non-padding positions to False\n",
    "    else:\n",
    "        # Otherwise, infer from the padding index\n",
    "        mask = (padded_input.squeeze(-1) == pad_idx)  # Shape (N, T)\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "def CausalMask(input_tensor):\n",
    "    \"\"\"\n",
    "    Create an attention mask for causal self-attention based on input lengths and target lengths.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): The input tensor of shape (N, T, *).\n",
    "\n",
    "    Returns:\n",
    "        attn_mask (torch.Tensor): The causal self-attention mask of shape (T, T)\n",
    "    \"\"\"\n",
    "    T = input_tensor.shape[1] # sequence length\n",
    "    # Self-attention mask: init with all padding\n",
    "    attn_mask   = torch.zeros(T, T, dtype=torch.bool)  # Shape (T, T)\n",
    "    causal_mask = ~torch.tril(torch.ones(T, T)).bool() # Lower triangular matrix\n",
    "    attn_mask   = attn_mask | causal_mask\n",
    "    # Return single-head attention mask without expanding for multi-head\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.pre_norm = torch.nn.LayerNorm(d_model)\n",
    "        self.self_attn = torch.nn.MultiheadAttention(embed_dim=d_model,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     dropout=dropout,\n",
    "                                                     batch_first=True)\n",
    "\n",
    "      \n",
    "        self.ffn1 = torch.nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "   \n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        # Self-attention with residual connection and normalization\n",
    "        residual = x\n",
    "        x = self.pre_norm(x)\n",
    "        att_out, att_weights = self.self_attn(query=x, key=x, value=x, need_weights=True, key_padding_mask=pad_mask, is_causal=False)\n",
    "        x = residual +  self.dropout(att_out)\n",
    "        x = self.norm1(x)\n",
    "        residual = x\n",
    "        # First FFN with residual connection and normalization\n",
    "        x = residual + self.dropout(self.ffn1(x))\n",
    "        x = self.norm2(x)\n",
    "        return x, pad_mask\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 max_len,\n",
    "                 target_vocab_size,\n",
    "                 dropout=0.1):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.inp_embedding       = torch.nn.Embedding(target_vocab_size, d_model)\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "       \n",
    "        self.after_norm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.ctc_head   = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        x = self.inp_embedding(x)\n",
    "        pad_mask = PadMask(padded_input=x, input_lengths=x_len).to(x.device)\n",
    "        residual = x\n",
    "        x = self.pos_encoding(x)\n",
    "        ## Apply Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Pass through encoder layers\n",
    "        x = x + residual\n",
    "        for layer in self.enc_layers:\n",
    "            x, pad_mask = layer(x, pad_mask)\n",
    "        x = self.after_norm(x)\n",
    "        x_ctc = self.ctc_head(x)\n",
    "        return x,  x_len, x_ctc.log_softmax(2).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # @TODO: fill in the blanks appropriately (given the modules above)\n",
    "        self.mha1       = nn.MultiheadAttention(embed_dim=d_model,\n",
    "                                                num_heads=num_heads,\n",
    "                                                dropout=dropout,\n",
    "                                                batch_first=True)\n",
    "        self.mha2       = nn.MultiheadAttention(embed_dim=d_model,\n",
    "                                                num_heads=num_heads,\n",
    "                                                dropout=dropout,\n",
    "                                                batch_first=True)\n",
    "        self.ffn        = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.identity   = nn.Identity()\n",
    "        self.pre_norm   = nn.LayerNorm(d_model)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1   = nn.Dropout(dropout)\n",
    "        self.dropout2   = nn.Dropout(dropout)\n",
    "        self.dropout3   = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, padded_targets, enc_output, pad_mask_enc, pad_mask_dec, slf_attn_mask):\n",
    "\n",
    "  \n",
    "        x = self.pre_norm(padded_targets)\n",
    "        residual = x\n",
    "        mha1_output, mha1_attn_weights = self.mha1(query=x,\n",
    "                                                   key=x,\n",
    "                                                   value=x,\n",
    "                                                   key_padding_mask=pad_mask_dec,\n",
    "                                                   need_weights=True,\n",
    "                                                   attn_mask=slf_attn_mask,\n",
    "                                                   average_attn_weights=True,\n",
    "                                                   is_causal=True)\n",
    "        mha1_output = residual + self.dropout1(mha1_output)\n",
    "        mha1_output = self.layernorm1(mha1_output)\n",
    "\n",
    "\n",
    "        residual = mha1_output\n",
    "\n",
    "        \n",
    "        mha2_output, mha2_attn_weights = self.mha2(query=mha1_output,\n",
    "                                                key=enc_output,\n",
    "                                                value=enc_output,\n",
    "                                                key_padding_mask=pad_mask_enc,\n",
    "                                                need_weights=True,\n",
    "                                                average_attn_weights=True,\n",
    "                                                is_causal=False)\n",
    "\n",
    "        output = residual + self.dropout2(mha2_output)\n",
    "        output = self.layernorm2(output)\n",
    "\n",
    "\n",
    "        ffn_output = self.ffn(output)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        ffn_output = ffn_output + output\n",
    "        ffn_output = self.layernorm3(ffn_output)\n",
    "\n",
    "\n",
    "        return ffn_output, mha1_attn_weights, mha2_attn_weights\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff, dropout,\n",
    "                 max_len,\n",
    "                 target_vocab_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len        = max_len\n",
    "        self.num_layers     = num_layers\n",
    "        self.num_heads      = num_heads\n",
    "\n",
    "        self.dec_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.target_embedding       = torch.nn.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding    = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.final_linear           = torch.nn.Linear(d_model, target_vocab_size)\n",
    "        self.dropout                = torch.nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, padded_targets, target_lengths, enc_output, enc_input_lengths):\n",
    "\n",
    "\n",
    "        pad_mask_dec = None\n",
    "        if target_lengths is not None:\n",
    "            pad_mask_dec = PadMask(padded_input=padded_targets, input_lengths=target_lengths).to(padded_targets.device)\n",
    "        causal_mask = CausalMask(input_tensor=padded_targets).to(padded_targets.device)\n",
    "        x = self.target_embedding(padded_targets)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "\n",
    "        pad_mask_enc = None\n",
    "        if enc_output is not None:\n",
    "          pad_mask_enc  = PadMask(padded_input=enc_output, input_lengths=enc_input_lengths).to(enc_output.device)\n",
    "\n",
    "  \n",
    "        runnint_att = {}\n",
    "        for i in range(self.num_layers):\n",
    "            x, runnint_att['layer{}_dec_self'.format(i + 1)], runnint_att['layer{}_dec_cross'.format(i + 1)] = self.dec_layers[i](padded_targets=x,\n",
    "                                                                                                                                enc_output=enc_output,\n",
    "                                                                                                                                pad_mask_enc=pad_mask_enc,\n",
    "                                                                                                                                pad_mask_dec=pad_mask_dec,\n",
    "                                                                                                                                slf_attn_mask=causal_mask)\n",
    "\n",
    "        seq_out = self.final_linear(x)\n",
    "        return seq_out, runnint_att\n",
    "\n",
    "\n",
    "    def recognize_greedy_search(self, enc_output, enc_input_lengths, target_output, target_lengths, tokenizer):\n",
    "\n",
    "        batch_size = target_output.size(0)\n",
    "        target_seq = torch.full((batch_size, 1), 0, dtype=torch.long).to(target_output.device)\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool).to(target_output.device)\n",
    "\n",
    "        for _ in range(self.max_len):\n",
    "\n",
    "            seq_out, runnint_att = self.forward(target_seq, None, enc_output, enc_input_lengths)\n",
    "            logits = torch.nn.functional.log_softmax(seq_out[:, -1], dim=1)\n",
    "\n",
    "\n",
    "            next_token = logits.argmax(dim=-1).unsqueeze(1)\n",
    "            target_seq = torch.cat([target_seq, next_token], dim=-1)\n",
    "            eos_mask = next_token.squeeze(-1) == 0\n",
    "            finished |= eos_mask\n",
    "            if finished.all(): break\n",
    "\n",
    "        target_seq = target_seq[:, 1:]\n",
    "        max_length = target_seq.size(1)\n",
    "        target_seq = torch.nn.functional.pad(target_seq,\n",
    "            (0, self.max_len - max_length), value=1)\n",
    "\n",
    "        return target_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "\n",
    "                 \n",
    "                 d_model,\n",
    "                 enc_num_layers,\n",
    "                 enc_num_heads,\n",
    "                 enc_max_len,\n",
    "                 enc_dropout,\n",
    "\n",
    "                 dec_num_layers,\n",
    "                 dec_num_heads,\n",
    "                 d_ff,\n",
    "                 dec_dropout,\n",
    "\n",
    "                 target_vocab_size,\n",
    "                 trans_max_len):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        self.encoder   = Encoder(\n",
    "            num_layers=enc_num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=enc_num_heads,\n",
    "            d_ff=d_ff,\n",
    "            max_len=enc_max_len,\n",
    "            target_vocab_size=target_vocab_size,\n",
    "            dropout=enc_dropout\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=dec_num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=dec_num_heads,\n",
    "            d_ff=d_ff,\n",
    "            max_len=trans_max_len,\n",
    "            target_vocab_size=target_vocab_size,\n",
    "            dropout=dec_dropout\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, padded_input, input_lengths, padded_target, target_lengths):\n",
    "        \n",
    "        \n",
    "        encoder_output, encoder_lengths, ctc_out = self.encoder(padded_input, input_lengths)\n",
    "        \n",
    "\n",
    "        output, attention_weights = self.decoder(padded_target, target_lengths, encoder_output, encoder_lengths)\n",
    "        return output, attention_weights, ctc_out\n",
    "\n",
    "\n",
    "    def recognize(self, inp, inp_len, target, target_len, tokenizer):\n",
    "        \n",
    "  \n",
    "        encoder_output, encoder_lengths, ctc_out = self.encoder(inp, inp_len)\n",
    "        \n",
    "\n",
    "        out = self.decoder.recognize_greedy_search(encoder_output, encoder_lengths, target, target_len, tokenizer=tokenizer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Transformer                                   [32, 121, 119547]         --\n",
       "├─Encoder: 1-1                                [32, 124, 256]            --\n",
       "│    └─Embedding: 2-1                         [32, 124, 256]            30,604,032\n",
       "│    └─PositionalEncoding: 2-2                [32, 124, 256]            --\n",
       "│    └─Dropout: 2-3                           [32, 124, 256]            --\n",
       "│    └─ModuleList: 2-4                        --                        --\n",
       "│    │    └─EncoderLayer: 3-1                 [32, 124, 256]            790,272\n",
       "│    │    └─EncoderLayer: 3-2                 [32, 124, 256]            790,272\n",
       "│    │    └─EncoderLayer: 3-3                 [32, 124, 256]            790,272\n",
       "│    │    └─EncoderLayer: 3-4                 [32, 124, 256]            790,272\n",
       "│    └─LayerNorm: 2-5                         [32, 124, 256]            512\n",
       "│    └─Linear: 2-6                            [32, 124, 119547]         30,723,579\n",
       "├─Decoder: 1-2                                [32, 121, 119547]         --\n",
       "│    └─Embedding: 2-7                         [32, 121, 256]            30,604,032\n",
       "│    └─PositionalEncoding: 2-8                [32, 121, 256]            --\n",
       "│    └─ModuleList: 2-9                        --                        --\n",
       "│    │    └─DecoderLayer: 3-5                 [32, 121, 256]            1,053,952\n",
       "│    │    └─DecoderLayer: 3-6                 [32, 121, 256]            1,053,952\n",
       "│    │    └─DecoderLayer: 3-7                 [32, 121, 256]            1,053,952\n",
       "│    │    └─DecoderLayer: 3-8                 [32, 121, 256]            1,053,952\n",
       "│    └─Linear: 2-10                           [32, 121, 119547]         30,723,579\n",
       "===============================================================================================\n",
       "Total params: 130,032,630\n",
       "Trainable params: 130,032,630\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.06\n",
       "===============================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 8067.69\n",
       "Params size (MB): 507.50\n",
       "Estimated Total Size (MB): 8575.25\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    \n",
    "    d_model        = config['d_model'],\n",
    "    enc_num_layers = config['enc_num_layers'],\n",
    "    enc_num_heads  = config['enc_num_heads'],\n",
    "    enc_max_len    = MAX_INP_LEN,\n",
    "    enc_dropout    = config['enc_dropout'],\n",
    "    dec_num_layers = config['dec_num_layers'],\n",
    "    dec_num_heads  = config['dec_num_heads'],\n",
    "    d_ff           = config['d_ff'],\n",
    "    dec_dropout    = config['dec_dropout'],\n",
    "    target_vocab_size = tokenizer.vocab_size,\n",
    "    trans_max_len     = MAX_TRANS_LEN\n",
    ")\n",
    "\n",
    "summary(model.to(device), input_data=[x_pad.to(device), x_len.to(device), y_shifted_pad.to(device), y_len.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.functional as aF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMetrics(reference, hypothesis):\n",
    "        dist = aF.edit_distance(reference, hypothesis)\n",
    "        ref_words = reference.split()\n",
    "        hyp_words = hypothesis.split()\n",
    "        dist = aF.edit_distance(ref_words, hyp_words)\n",
    "        wer = dist / len(ref_words)\n",
    "        ref_chars = list(reference)\n",
    "        hyp_chars = list(hypothesis)\n",
    "        dist = aF.edit_distance(ref_chars, hyp_chars)\n",
    "        cer = dist / len(ref_chars)\n",
    "        return dist, wer, cer\n",
    "\n",
    "\n",
    "def calculateBatchMetrics(predictions, y, y_len, tokenizer):\n",
    "    '''\n",
    "    Calculate levenshtein distance, WER, CER for a batch\n",
    "    predictions (Tensor) : the model predictions\n",
    "    y (Tensor) : the target transcript\n",
    "    y_len (Tensor) : Length of the target transcript (non-padded positions)\n",
    "    '''\n",
    "    batch_size, _  = predictions.shape\n",
    "    dist, wer, cer = 0., 0., 0.\n",
    "    for batch_idx in range(batch_size):\n",
    "\n",
    "        pad_indices = torch.where(predictions[batch_idx] == 1)[0]\n",
    "        if pad_indices.numel() > 0:\n",
    "            lowest_pad_idx = pad_indices.min().item()\n",
    "        else:\n",
    "            lowest_pad_idx = len(predictions[batch_idx])\n",
    "        pred_trimmed = predictions[batch_idx, :lowest_pad_idx]\n",
    "\n",
    "\n",
    "        y_trimmed   = y[batch_idx, 0 : y_len[batch_idx]-1]\n",
    "\n",
    "        pred_string  = tokenizer.decode(pred_trimmed)\n",
    "        y_string     = tokenizer.decode(y_trimmed)\n",
    "\n",
    "        curr_dist, curr_wer, curr_cer = calculateMetrics(y_string, pred_string)\n",
    "        dist += curr_dist\n",
    "        wer  += curr_wer\n",
    "        cer  += curr_cer\n",
    "\n",
    "    dist /= batch_size\n",
    "    wer  /= batch_size\n",
    "    cer  /= batch_size\n",
    "    return dist, wer, cer, y_string, pred_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_attention_plot(plot_path, attention_weights, epoch=0):\n",
    "    ''' function for saving attention weights plot to a file\n",
    "\n",
    "        @NOTE: default starter code set to save cross attention\n",
    "    '''\n",
    "    plt.clf() \n",
    "    sns.heatmap(attention_weights, cmap=\"viridis\")  # Create heatmap\n",
    "\n",
    "    if epoch<100:\n",
    "        plt.savefig(f\"{plot_path}/cross_attention-epoch{epoch}.png\")\n",
    "    else :\n",
    "        plt.savefig(f\"{plot_path}/self_attention-epoch{epoch-100}.png\")\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "      torch.save(\n",
    "          {\"model_state_dict\"        : model.state_dict(),\n",
    "          \"optimizer_state_dict\"     : optimizer.state_dict(),\n",
    "          \"scheduler_state_dict\"     : scheduler.state_dict() if scheduler is not None else {},\n",
    "          metric[0]                  : metric[1],\n",
    "          \"epoch\"                    : epoch},\n",
    "          path\n",
    "      )\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path,\n",
    "                    model,\n",
    "                    embedding_load:bool,\n",
    "                    encoder_load:bool,\n",
    "                    decoder_load:bool):\n",
    " \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    embedding_state_dict = {k: v for k, v in checkpoint['model_state_dict'].items() if k.startswith(\"embedding\")}\n",
    "    encoder_state_dict   = {k: v for k, v in checkpoint['model_state_dict'].items() if k.startswith(\"encoder\")}\n",
    "    decoder_state_dict   = {k: v for k, v in checkpoint['model_state_dict'].items() if k.startswith(\"decoder\")}\n",
    "\n",
    "    if embedding_load:\n",
    "      model_state_dict.update(embedding_state_dict)\n",
    "    if encoder_load:\n",
    "      model_state_dict.update(encoder_state_dict)\n",
    "    if decoder_load:\n",
    "      model_state_dict.update(decoder_state_dict)\n",
    "\n",
    "    model.load_state_dict(model_state_dict, strict=False)\n",
    "    s0 = \"Embedding\" if embedding_load else \"\"\n",
    "    s1 = \"Encoder\" if encoder_load else \"\"\n",
    "    s2 = \"Decoder\" if decoder_load else \"\"\n",
    "    print(\"Loaded: \", f'{s0}, {s1}, {s2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,\n",
    "               criterion,\n",
    "               ctc_loss,\n",
    "               ctc_weight,\n",
    "               optimizer,\n",
    "               scheduler,\n",
    "               scaler,\n",
    "               device,\n",
    "               train_loader,\n",
    "               tokenizer):\n",
    "\n",
    "        model.train()\n",
    "        batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0)\n",
    "\n",
    "        running_loss        = 0.0\n",
    "        running_perplexity  = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "           \n",
    "            inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths, _ = batch\n",
    "            inputs          = inputs.to(device)\n",
    "        \n",
    "            targets_shifted = targets_shifted.to(device)\n",
    "            targets_golden  = targets_golden.to(device)\n",
    "\n",
    "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                raw_predictions, attention_weights, ctc_out = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
    "                padding_mask = torch.logical_not(torch.eq(targets_shifted, 1))\n",
    "\n",
    "                loss = criterion(raw_predictions.transpose(1,2), targets_golden)*padding_mask\n",
    "                loss = loss.sum() / padding_mask.sum()\n",
    "\n",
    "                if ctc_loss is not None and ctc_out is not None:\n",
    "                  inputs_lengths = torch.ceil(inputs_lengths.float() / model.embedding.time_downsampling_factor).int()\n",
    "                  inputs_lengths = inputs_lengths.clamp(max=ctc_out.size(0))\n",
    "                  loss = loss + ctc_weight * ctc_loss(ctc_out, targets_golden, inputs_lengths, targets_lengths)\n",
    "\n",
    "            scaler.scale(loss).backward() \n",
    "            scaler.step(optimizer)        \n",
    "            scaler.update()             \n",
    "            perplexity           = torch.exp(loss)\n",
    "            running_perplexity  += perplexity.item()\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                loss = \"{:.04f}\".format(float(running_loss / (i + 1))),\n",
    "                perplexity = \"{:.04f}\".format(float(running_perplexity / (i + 1)))\n",
    "            )\n",
    "\n",
    "            batch_bar.update()\n",
    "\n",
    "            del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        running_loss        = float(running_loss / len(train_loader))\n",
    "        running_perplexity  = float(running_perplexity / len(train_loader))\n",
    "        batch_bar.close()\n",
    "        return running_loss, running_perplexity, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "def validate_step(model, val_loader, tokenizer, device,  threshold=5):\n",
    "      model.eval()\n",
    "      batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
    "\n",
    "      running_distance = 0.0\n",
    "      running_wer = 0.0\n",
    "      running_cer = 0.0\n",
    "      json_output = {}\n",
    "\n",
    "      for i, batch in enumerate(val_loader):\n",
    "\n",
    "          \n",
    "            inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths, _ = batch\n",
    "            inputs          = inputs.to(device)\n",
    "            \n",
    "            targets_shifted = targets_shifted.to(device)\n",
    "            targets_golden  = targets_golden.to(device)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                greedy_predictions = model.recognize(inputs, inputs_lengths, targets_shifted, targets_lengths, tokenizer=tokenizer)\n",
    "\n",
    "            dist, wer, cer, y_string, pred_string = calculateBatchMetrics(greedy_predictions, targets_golden, targets_lengths, tokenizer)\n",
    "            running_distance += dist\n",
    "            running_wer += wer\n",
    "            running_cer += cer\n",
    "\n",
    "            json_output[i] = {\n",
    "                \"Input\": y_string,\n",
    "                \"Output\": pred_string\n",
    "            }\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                running_distance = \"{:.04f}\".format(float(running_distance / (i + 1))),\n",
    "                WER = \"{:.04f}\".format(float(running_wer / (i + 1))),\n",
    "                CER = \"{:.04f}\".format(float(running_cer / (i + 1)))\n",
    "            )\n",
    "\n",
    "            batch_bar.update()\n",
    "\n",
    "            del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if threshold is not None:\n",
    "                if i==threshold: break      # validating only upon first five batches\n",
    "\n",
    "      batch_bar.close()\n",
    "      if threshold is not None:\n",
    "        running_distance /= (threshold+1)\n",
    "        running_wer /= (threshold+1)\n",
    "        running_cer /= (threshold+1)\n",
    "      else:\n",
    "        running_distance /= len(val_loader)\n",
    "        running_wer /= len(val_loader)\n",
    "        running_cer /= len(val_loader)\n",
    "\n",
    "      return running_distance, json_output, running_wer, running_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_449302/2603785494.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler      = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "loss_func   = nn.CrossEntropyLoss(ignore_index = 1)\n",
    "ctc_loss_fn  = None\n",
    "if config['use_ctc']:\n",
    "  ctc_loss_fn = nn.CTCLoss(blank=1)\n",
    "scaler      = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def get_optimizer():\n",
    "    optimizer = None\n",
    "    if config[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                    lr=config[\"learning_rate\"],\n",
    "                                    momentum=config[\"momentum\"],\n",
    "                                    weight_decay=1E-4,\n",
    "                                    nesterov=config[\"nesterov\"])\n",
    "\n",
    "    elif config[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                    lr=float(config[\"learning_rate\"]),weight_decay=0.01 )\n",
    "\n",
    "    elif config[\"optimizer\"] == \"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                    lr=float(config[\"learning_rate\"]),\n",
    "                                    weight_decay=0.01)\n",
    "    return optimizer\n",
    "optimizer = get_optimizer()\n",
    "assert optimizer!=None\n",
    "\n",
    "def get_scheduler():\n",
    "    scheduler  =  None\n",
    "    if config[\"scheduler\"] == \"ReduceLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                        factor=config[\"factor\"], patience=config[\"patience\"], min_lr=1E-8, threshold=1E-1)\n",
    "\n",
    "    elif config[\"scheduler\"] == \"CosineAnnealing\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                        T_max = config[\"epochs\"], eta_min=1E-8)\n",
    "    return scheduler\n",
    "\n",
    "scheduler = get_scheduler()\n",
    "assert scheduler!=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1607"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import shutil\n",
    "USE_WANDB = False\n",
    "RESUME_LOGGING = False\n",
    "\n",
    "\n",
    "run_name = \"Proj-{}-{}_DEC-{}-{}_{}_{}_{}_{}_token_greedy\".format(\n",
    "    config[\"Name\"],\n",
    "    config[\"enc_num_layers\"],\n",
    "    config[\"enc_num_heads\"],\n",
    "    config[\"dec_num_layers\"],\n",
    "    config[\"dec_num_heads\"],\n",
    "    config[\"d_model\"],\n",
    "    config[\"d_ff\"],\n",
    "    config[\"optimizer\"],\n",
    "    config[\"scheduler\"],\n",
    "   \n",
    "    )\n",
    "\n",
    "expt_root = os.path.join(os.getcwd(), run_name)\n",
    "os.makedirs(expt_root, exist_ok=True)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login(key=\"enter_your_key\", relogin=True) \n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = \"\"\n",
    "        run = wandb.init(\n",
    "            id     = run_id,        \n",
    "            resume = True,         \n",
    "            project = \"IDL Project\",  \n",
    "        )\n",
    "\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name,    \n",
    "            reinit  = True,       \n",
    "            project = \"Project\", \n",
    "            config  = config       \n",
    "        )\n",
    "\n",
    "        model_arch  = str(model)\n",
    "        ### Save it in a txt file\n",
    "        model_path = os.path.join(expt_root, \"model_arch.txt\")\n",
    "        arch_file   = open(model_path, \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "\n",
    "shutil.copy(os.path.join(os.getcwd(), 'config.yaml'), os.path.join(expt_root, 'config.yaml'))\n",
    "e                   = 0\n",
    "best_loss           = 10.0\n",
    "best_perplexity     = 10.0\n",
    "best_dist = 60\n",
    "RESUME_LOGGING = False\n",
    "checkpoint_root = os.path.join(expt_root, 'checkpoints')\n",
    "text_root       = os.path.join(expt_root, 'out_text')\n",
    "attn_img_root   = os.path.join(expt_root, 'attention_imgs')\n",
    "os.makedirs(checkpoint_root, exist_ok=True)\n",
    "os.makedirs(attn_img_root,   exist_ok=True)\n",
    "os.makedirs(text_root,       exist_ok=True)\n",
    "checkpoint_best_loss_model_filename     = 'checkpoint-best-loss-modelfull.pth'\n",
    "checkpoint_last_epoch_filename          = 'checkpoint-epochfull-'\n",
    "best_loss_model_path                    = os.path.join(checkpoint_root, checkpoint_best_loss_model_filename)\n",
    "\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.watch(model, log=\"all\")\n",
    "\n",
    "if RESUME_LOGGING:\n",
    "    # change if you want to load best test model accordingly\n",
    "    checkpoint = torch.load(wandb.restore(checkpoint_best_loss_model_filename, run_path=\"\"+run_id).name)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    e = checkpoint['epoch']\n",
    "\n",
    "    print(\"Resuming from epoch {}\".format(e+1))\n",
    "    print(\"Epochs left: \", config['epochs']-e)\n",
    "    print(\"Optimizer: \\n\", optimizer)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100: \n",
      "Train Loss 2.1268\t Train Perplexity 8.7983\t Learning Rate 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100: \n",
      "Train Loss 1.3489\t Train Perplexity 3.9324\t Learning Rate 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100: \n",
      "Train Loss 0.7955\t Train Perplexity 2.2381\t Learning Rate 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100: \n",
      "Train Loss 0.4339\t Train Perplexity 1.5495\t Learning Rate 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100: \n",
      "Train Loss 0.2359\t Train Perplexity 1.2671\t Learning Rate 0.000199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100: \n",
      "Train Loss 0.1420\t Train Perplexity 1.1529\t Learning Rate 0.000199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100: \n",
      "Train Loss 0.0934\t Train Perplexity 1.0980\t Learning Rate 0.000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/100: \n",
      "Train Loss 0.0684\t Train Perplexity 1.0708\t Learning Rate 0.000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/100: \n",
      "Train Loss 0.0535\t Train Perplexity 1.0550\t Learning Rate 0.000197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100: \n",
      "Train Loss 0.0434\t Train Perplexity 1.0444\t Learning Rate 0.000196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100: \n",
      "Train Loss 0.0364\t Train Perplexity 1.0371\t Learning Rate 0.000195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/100: \n",
      "Train Loss 0.0309\t Train Perplexity 1.0314\t Learning Rate 0.000194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/100: \n",
      "Train Loss 0.0271\t Train Perplexity 1.0275\t Learning Rate 0.000193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n",
      "\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/100: \n",
      "Train Loss 0.0239\t Train Perplexity 1.0242\t Learning Rate 0.000192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance : 82.0365\n",
      "WER                  : 1.0000\n",
      "CER                  : 0.9759\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\u001b[39;00m\n\u001b[1;32m     68\u001b[0m epoch_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_root, (checkpoint_last_epoch_filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 69\u001b[0m save_model(model, optimizer, scheduler, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, train_loss], epoch, epoch_model_path)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_dist \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m levenshtein_distance:\n\u001b[1;32m     72\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m train_loss\n",
      "Cell \u001b[0;32mIn[98], line 18\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, optimizer, scheduler, metric, epoch, path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model\u001b[39m(model, optimizer, scheduler, metric, epoch, path):\n\u001b[0;32m---> 18\u001b[0m       torch\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     19\u001b[0m           {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m        : model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     20\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m     : optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     21\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m     : scheduler\u001b[38;5;241m.\u001b[39mstate_dict() \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[1;32m     22\u001b[0m           metric[\u001b[38;5;241m0\u001b[39m]                  : metric[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     23\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m                    : epoch},\n\u001b[1;32m     24\u001b[0m           path\n\u001b[1;32m     25\u001b[0m       )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/serialization.py:883\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 883\u001b[0m     storage \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/storage.py:167\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "for epoch in range(e, epochs):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config[\"epochs\"]))\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    train_loss, train_perplexity, attention_weights = train_step(\n",
    "        model,\n",
    "        criterion=loss_func,\n",
    "        ctc_loss=ctc_loss_fn,\n",
    "        ctc_weight=config['ctc_weight'],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Train Perplexity {:.04f}\\t Learning Rate {:.06f}\".format(\n",
    "        epoch + 1, config[\"epochs\"], train_loss, train_perplexity, curr_lr))\n",
    "\n",
    "\n",
    "    levenshtein_distance, json_out, wer, cer = validate_step(\n",
    "        model,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        threshold=5\n",
    "    )\n",
    "\n",
    "\n",
    "    fpath = os.path.join(text_root, f'full_{epoch+1}_out.json')\n",
    "    with open(fpath, \"w\") as f:\n",
    "        json.dump(json_out, f, indent=4)\n",
    "\n",
    "    print(\"Levenshtein Distance : {:.04f}\".format(levenshtein_distance))\n",
    "    print(\"WER                  : {:.04f}\".format(wer))\n",
    "    print(\"CER                  : {:.04f}\".format(cer))\n",
    "\n",
    "    attention_keys = list(attention_weights.keys())\n",
    "    attention_weights_decoder_self   = attention_weights[attention_keys[0]][0].cpu().detach().numpy()\n",
    "    attention_weights_decoder_cross  = attention_weights[attention_keys[-1]][0].cpu().detach().numpy()\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            \"train_loss\"       : train_loss,\n",
    "            \"train_perplexity\" : train_perplexity,\n",
    "            \"learning_rate\"    : curr_lr,\n",
    "            \"lev_dist\"         : levenshtein_distance,\n",
    "            \"WER\"              : wer,\n",
    "            \"CER\"              : cer\n",
    "        })\n",
    "\n",
    "\n",
    "    save_attention_plot(str(attn_img_root), attention_weights_decoder_cross, epoch )\n",
    "    save_attention_plot(str(attn_img_root), attention_weights_decoder_self, epoch+100)\n",
    "    if config[\"scheduler\"] == \"ReduceLR\":\n",
    "        scheduler.step(levenshtein_distance)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_model_path = os.path.join(checkpoint_root, (checkpoint_last_epoch_filename + str(epoch) + '.pth'))\n",
    "    save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, epoch_model_path)\n",
    "\n",
    "    if best_dist >= levenshtein_distance:\n",
    "        best_loss = train_loss\n",
    "        best_dist = levenshtein_distance\n",
    "        save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, best_loss_model_path)\n",
    "        print(\"Saved best distance model\")\n",
    "\n",
    "\n",
    "if USE_WANDB:\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "processor = AutoProcessor.from_pretrained(\"mbazaNLP/Whisper-Small-Kinyarwanda\")\n",
    "model_whisper = AutoModelForSpeechSeq2Seq.from_pretrained(\"mbazaNLP/Whisper-Small-Kinyarwanda\")\n",
    "model_whisper.config.forced_decoder_ids = None\n",
    "model_whisper.config.suppress_tokens = None\n",
    "\n",
    "whisper_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_whisper,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "\n",
    "    device=\"cuda\",   \n",
    "\n",
    ")\n",
    "\n",
    "def predict(model, audio, tokenizer):\n",
    "    out = whisper_pipe(audio)\n",
    "\n",
    "    noisy_text = out['text']\n",
    "    model.eval()\n",
    "    input = tokenizer.encode(noisy_text)\n",
    "    greedy_predictions = model.recognize(input, [len(input)], tokenizer=tokenizer)\n",
    "\n",
    "   \n",
    "    \n",
    "    pad_indices = torch.where(greedy_predictions[0] == 1)[0]\n",
    "    if pad_indices.numel() > 0:\n",
    "        lowest_pad_idx = pad_indices.min().item()\n",
    "    else:\n",
    "        lowest_pad_idx = len(greedy_predictions[0])\n",
    "    pred_trimmed = greedy_predictions[0, :lowest_pad_idx]\n",
    "    \n",
    "    pred_string  = tokenizer.decode(pred_trimmed)\n",
    "\n",
    "\n",
    "\n",
    "    return pred_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create data for transcripts\n",
    "transcripts = [\n",
    "    \"Imiti yo kuvura kanseri ikunze kugira ingaruka zikomeye ku murwayi.\",\n",
    "    \"Guhuza imirire myiza n'imyitozo ngororamubiri byongera ubuzima bwiza.\",\n",
    "    \"Ubwandu bw'igituntu bukomeje kuba ikibazo cy'ubuzima rusange mu bihugu biri mu nzira y'amajyambere.\",\n",
    "    \"Ubuvuzi bw'amenyo bukwiye guhabwa umwanya uhagije mu buvuzi rusange.\",\n",
    "    \"Urukingo rwa COVID-19 rwagaragaje umusaruro mwiza mu kugabanya ubukana.\",\n",
    "    \"Ubushakashatsi bwerekana ko kugabanya umunyu bigabanya ibyago byo kurwara umutima.\",\n",
    "    \"Kubyara kare bishobora kugira ingaruka mbi ku buzima bw'umubyeyi n'umwana.\",\n",
    "    \"Abantu benshi batitabira gupimwa indwara zitandura nka diyabete.\",\n",
    "    \"Ubuvuzi bwa kinyarwanda bufite umwanya wihariye mu muryango nyarwanda.\",\n",
    "    \"Gukoresha ikoranabuhanga mu buvuzi byongereye ubudahangarwa mu kwita ku barwayi.\",\n",
    "    \"Imibereho y'abantu igira uruhare runini mu gutegura ubuvuzi bukwiye.\",\n",
    "    \"Kurwanya malariya bisaba ubufatanye bw'inzego zose z'ubuzima.\",\n",
    "    \"Ubusumbane mu buvuzi bukomeje kuba imbogamizi ikomeye.\",\n",
    "    \"Itabi rikomeje kuba intandaro y'indwara z'umutima n'ibihaha.\",\n",
    "    \"Kwita ku buzima bwo mu mutwe bikwiye kuba mu by'ibanze mu buvuzi.\",\n",
    "    \"Imirire idahwitse ikomeje kuba ikibazo mu bana bato.\",\n",
    "    \"Kuboneza urubyaro ni ingenzi mu kugabanya umubare w'abaturage.\",\n",
    "    \"Ubukangurambaga ku buzima bw'imyororokere burakenewe mu rubyiruko.\",\n",
    "    \"Amavuriro y'ibanze akwiye guhabwa ibikoresho bihagije.\",\n",
    "    \"Ubuvuzi bw'amatungo bufitanye isano ya hafi n'ubuzima bw'abantu.\",\n",
    "    \"Kugabanya ibiro birakenewe ku bantu bafite umubyibuho ukabije.\",\n",
    "    \"Uruhare rw'ubumenyi mu gukumira no kuvura indwara ntirugomba gusuzugurwa.\",\n",
    "    \"Kugira ubuzima bwiza bisaba kumenya no kwirinda indwara z'ibyorezo.\",\n",
    "    \"Ubufasha bw'ibanze buhutiraho bushobora kurokora ubuzima.\",\n",
    "    \"Amazi meza n'isuku ni ingenzi mu kwirinda indwara ziterwa n'umwanda.\",\n",
    "    \"Indwara z'umutima zikomeje kwiyongera mu baturage.\",\n",
    "    \"Uburezi bujyanye n'ubuzima bukenewe guhera mu mashuri abanza.\",\n",
    "    \"Gutanga amahugurwa ku baganga bikwiye guhoraho.\",\n",
    "    \"Gupima indwara hakiri kare ni ingenzi mu kuzivura neza.\",\n",
    "    \"Kurwanya umubyibuho ukabije ni kimwe mu bibangamiye ubuzima rusange.\",\n",
    "    \"Ubushakashatsi mu buvuzi bukeneye inkunga y'ibihugu.\",\n",
    "    \"Uruhare rw'imyitozo ngororamubiri mu kwirinda indwara ntirukwiye kwirengagizwa.\",\n",
    "    \"Imiti gakondo ishobora kunganira ubuvuzi bugezweho.\",\n",
    "    \"Umuvuduko w'amaraso ukabije ugomba gukurikiranwa n'abaganga.\",\n",
    "    \"Kugabanya isukari mu mirire ni ingenzi mu kurwanya diyabete.\",\n",
    "    \"Abantu bakwiye gushishikarizwa gukoresha urukingo rw'igituntu.\",\n",
    "    \"Ubuvuzi bw'ibyorezo bikomeye bukwiye gutegurwa neza mu bihugu.\",\n",
    "    \"Ubushakashatsi ku ndwara z'ibyorezo burakenewe kugira ngo tubashe kuzitsinda.\",\n",
    "    \"Imiti irwanya virusi igomba gutangwa ku barwayi ku gihe.\",\n",
    "    \"Kuvura ibikomere bikomeje kuba ikibazo mu bihe by'intambara.\",\n",
    "    \"Gukurikirana ubuzima bw'abagore batwite ni ingenzi mu kugabanya impfu z'ababyeyi.\",\n",
    "    \"Guteza imbere ubuvuzi bwa telemedicine byagize akamaro mu bihe bya COVID-19.\",\n",
    "    \"Abana bato bakwiye guhabwa inkingo zose ziteganyijwe.\",\n",
    "    \"Kwirinda indwara zitandura ni kimwe mu bibazo by'ubuzima bw'ibanze.\",\n",
    "    \"Gukumira indwara z'umutima bisaba ubufatanye bw'abaturage n'abaganga.\",\n",
    "    \"Kurwanya indwara ya hepatitis bisaba ubukangurambaga bukomeye.\",\n",
    "    \"Kuboneza imirire y'abana bato ni ngombwa mu kurwanya bwaki.\",\n",
    "    \"Uruhare rw'abaturage mu kubungabunga ubuzima bwabo ntirugomba gusuzugurwa.\",\n",
    "    \"Kwita ku bana bafite ubumuga bukomatanyije bisaba ubufasha bwihariye.\",\n",
    "    \"Gukurikirana iterambere ry'ubuvuzi bugezweho ni ingenzi mu kurwanya indwara.\",\n",
    "    \"Kuvura indwara zifata amagufa bisaba ubuhanga n'ubushishozi.\",\n",
    "    \"Gushyira imbere ubushakashatsi ku ndwara zidakira ni ingenzi.\",\n",
    "    \"Gukumira no kuvura malariya bisaba gukoresha inzitiramibu mu ngo.\",\n",
    "    \"Kwirinda no kuvura indwara z'umwijima bikwiye gufatwa nk'ingenzi.\",\n",
    "    \"Kugabanya umunaniro ukabije mu bakozi ni ngombwa mu rwego rw'ubuzima.\",\n",
    "    \"Kwita ku isuku y'ibikoresho byo mu bitaro ni ingenzi mu kwirinda indwara.\",\n",
    "    \"Guteza imbere uburezi bujyanye n'ubuvuzi bw'amaso ni ngombwa.\",\n",
    "    \"Uruhare rw'abaganga mu kuvura indwara zo mu mutwe ntirugomba kwirengagizwa.\",\n",
    "    \"Gushyira imbere ubuvuzi bwa kanseri y'ibere ni ingenzi.\",\n",
    "    \"Ubukangurambaga ku buzima bw'imyororokere burakenewe mu mashuri.\"\n",
    "]\n",
    "\n",
    "# Generate recording names in the format rec1, rec2, ..., rec60\n",
    "recordings = [f\"/kaggle/input/testsss-dataset/rec{i+1}.wav\" for i in range(60)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'recording': recordings,\n",
    "    'transcript': transcripts\n",
    "})\n",
    "\n",
    "# # Save DataFrame to CSV\n",
    "# df.to_csv('/kaggle/working/med_test',index=False)\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('/kaggle/working/medicall_tests',index=False)\n",
    "df = pd.read_csv(\"/kaggle/working/medicall_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean WER: 24.3400\n",
      "Mean CER: 10.6200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from jiwer import wer, cer\n",
    "\n",
    "wer_errors = []\n",
    "cer_errors = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    result = predict(model,row['recording'],tokenizer)\n",
    "    wer_error = wer(row['transcript'], result['text'])\n",
    "    cer_error = cer(row['transcript'], result['text'])\n",
    "    wer_errors.append(wer_error)\n",
    "    cer_errors.append(cer_error)\n",
    "    print(f\"Processing recording {idx+1}/60\")  # Progress tracking\n",
    "\n",
    "mean_wer = np.mean(wer_errors)\n",
    "mean_cer = np.mean(cer_errors)\n",
    "\n",
    "print(f\"\\nMean WER: {mean_wer:.4f}\")\n",
    "print(f\"Mean CER: {mean_cer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
